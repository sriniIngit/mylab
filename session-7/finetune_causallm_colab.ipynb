{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-7/finetune_causallm_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fd592d-0ad0-4f1b-94f4-0e5ae9dbeec7",
      "metadata": {
        "id": "a9fd592d-0ad0-4f1b-94f4-0e5ae9dbeec7"
      },
      "source": [
        "# Fine-Tune a Causal Language Model for Dialogue Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "976f3255-166f-4e6f-aaf0-ef6c4703b9f7",
      "metadata": {
        "id": "976f3255-166f-4e6f-aaf0-ef6c4703b9f7"
      },
      "source": [
        "In this exercise, you will fine-tune Meta's Llama 2 for enhanced dialogue summarization. Llama 2 is a large language model (LLM) free for research and commercial use. It is one of the top-performing open-source LLM  comparable to GPT-3.5 on several benchmarks.\n",
        "\n",
        "We will explore the use of Parameter Efficient Fine-Tuning (PEFT) for fine-tuning, and evaluate the resulting model using ROUGE metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3b6ce61-1b18-412e-8ea9-b1aa4b6ea33d",
      "metadata": {
        "id": "a3b6ce61-1b18-412e-8ea9-b1aa4b6ea33d"
      },
      "source": [
        "## Install the pre-requisites\n",
        "\n",
        "Uncomment the following if these python packages have not been installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da7e887-227f-4cce-983a-83ab3405de65",
      "metadata": {
        "tags": [],
        "id": "4da7e887-227f-4cce-983a-83ab3405de65"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate sentencepiece scipy peft bitsandbytes evaluate rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a39feac3-3c0f-4057-83e2-898a0329cbee",
      "metadata": {
        "id": "a39feac3-3c0f-4057-83e2-898a0329cbee"
      },
      "source": [
        "## Request access to Llama-2 weights\n",
        "\n",
        "You need to request for access to download the Llama 2 weights. You can either do so through this [link at Meta](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) or through your huggingface account at this [link](https://huggingface.co/meta-llama/Llama-2-7b). Once your request is approved, you will receive an email from Meta with instruction to download the Llama 2 weights, or email from Hugging Face informing you access has been granted.\n",
        "\n",
        "If you download the weights from Meta directly, you need to run a conversion script to convert the weights to huggingface format for use with huggingface transformer library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4178ec51-ccf8-4d01-8224-e57c8b0683f1",
      "metadata": {
        "tags": [],
        "id": "4178ec51-ccf8-4d01-8224-e57c8b0683f1"
      },
      "outputs": [],
      "source": [
        "# %%bash\n",
        "# TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weights_to_hf.py')\"`\n",
        "# python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00149073-36b6-4317-818b-c8e0485cd8d2",
      "metadata": {
        "tags": [],
        "id": "00149073-36b6-4317-818b-c8e0485cd8d2"
      },
      "outputs": [],
      "source": [
        "# Uncomment the following to login to HuggingFace to access the Llama model (only need to do once)\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4442511b-f066-470a-8ef2-f9d59c1a8733",
      "metadata": {
        "id": "4442511b-f066-470a-8ef2-f9d59c1a8733"
      },
      "source": [
        "## Import packages\n",
        "\n",
        "We first import all the necessary python libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import default_data_collator, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "In8vYch5vbzc"
      },
      "id": "In8vYch5vbzc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "eb3aa994-e650-4b39-8bf9-5d888e2f3088",
      "metadata": {
        "id": "eb3aa994-e650-4b39-8bf9-5d888e2f3088"
      },
      "source": [
        "## Load the Pretrained Model and Tokenizer\n",
        "\n",
        "Load the pre-trained Llama 2 model and its tokenizer directly from HuggingFace. We will load the model in 8 bit quantization to save memory. For a more detailed understanding about how the model perform the matrix multiplication in 8-bit, see this [blog post](https://huggingface.co/blog/hf-bitsandbytes-integration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93573bed-1060-481d-b75a-2ae0eac46e2f",
      "metadata": {
        "tags": [],
        "id": "93573bed-1060-481d-b75a-2ae0eac46e2f"
      },
      "outputs": [],
      "source": [
        "model_id=\"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='cuda:0', use_cache=False)\n",
        "# model = LlamaForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e48bfd9-9813-47e1-ba0e-ee08722f893f",
      "metadata": {
        "id": "3e48bfd9-9813-47e1-ba0e-ee08722f893f"
      },
      "source": [
        "The following shows the GPU memory consumption on an A10G GPU, with different model dtype.\n",
        "\n",
        "- load_in_8bit = 7512 MB\n",
        "- load_in_16bit = 13174 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa2b8f5-bd17-4e09-a4c3-c87ae5c825e3",
      "metadata": {
        "tags": [],
        "id": "afa2b8f5-bd17-4e09-a4c3-c87ae5c825e3"
      },
      "outputs": [],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9574b5dd-8ba7-4fa9-944f-646bd49c62f4",
      "metadata": {
        "tags": [],
        "id": "9574b5dd-8ba7-4fa9-944f-646bd49c62f4"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "We are going to use the DialogSum Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. Note that the dataset is already split into train, validation and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c26a2ef-ed9c-4d03-acb5-2b9fc1224be7",
      "metadata": {
        "tags": [],
        "id": "4c26a2ef-ed9c-4d03-acb5-2b9fc1224be7"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(dataset_name)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fdbe4df-2e63-4f7f-864f-0fddbd7c370e",
      "metadata": {
        "tags": [],
        "id": "1fdbe4df-2e63-4f7f-864f-0fddbd7c370e"
      },
      "outputs": [],
      "source": [
        "dataset_train = dataset['train']\n",
        "dataset_test = dataset['test']\n",
        "dataset_val = dataset['validation']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5b7da07-9192-4718-b10e-c47fa5634c02",
      "metadata": {
        "id": "f5b7da07-9192-4718-b10e-c47fa5634c02"
      },
      "source": [
        "Let's taka a look at one of the samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ce8a86-3bc2-44d6-9cf1-3dcda2085ede",
      "metadata": {
        "tags": [],
        "id": "f1ce8a86-3bc2-44d6-9cf1-3dcda2085ede"
      },
      "outputs": [],
      "source": [
        "dataset['train'][100]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8358e2e-e29d-4d5c-89f9-f78959ef5527",
      "metadata": {
        "id": "e8358e2e-e29d-4d5c-89f9-f78959ef5527"
      },
      "source": [
        "## Test the Model with Zero Shot Inferencing\n",
        "\n",
        "Let's test the model with zero shot inferencing (i.e. ask it to summarize without giving any example. You can see that the model struggles to summarize the dialogue compared to the baseline summary, and it is just repeating the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31def3d5-e863-4b34-8c2e-408a0cb8f0b4",
      "metadata": {
        "tags": [],
        "id": "31def3d5-e863-4b34-8c2e-408a0cb8f0b4"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"\n",
        "Summarize this dialog:\n",
        "#Person1#: I have a problem with my cable.\n",
        "#Person2#: What about it?\n",
        "#Person1#: My cable has been out for the past week or so.\n",
        "#Person2#: The cable is down right now. I am very sorry.\n",
        "#Person1#: When will it be working again?\n",
        "#Person2#: It should be back on in the next couple of days.\n",
        "#Person1#: Do I still have to pay for the cable?\n",
        "#Person2#: We're going to give you a credit while the cable is down.\n",
        "#Person1#: So, I don't have to pay for it?\n",
        "#Person2#: No, not until your cable comes back on.\n",
        "#Person1#: Okay, thanks for everything.\n",
        "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
        "---\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():   # no gradient update\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=200)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf285722-2ac5-4e60-8cc6-06c048e6d723",
      "metadata": {
        "id": "cf285722-2ac5-4e60-8cc6-06c048e6d723"
      },
      "source": [
        "## Creating instruction dataset\n",
        "\n",
        "We will now prepare our dataset to fine-tune our base model (instruction fine-tuning)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4012814-8355-4d75-8d0e-e1efbd2ba8de",
      "metadata": {
        "tags": [],
        "id": "e4012814-8355-4d75-8d0e-e1efbd2ba8de"
      },
      "source": [
        "### Instruction prompt\n",
        "\n",
        "We need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM such as follows:\n",
        "\n",
        "```\n",
        "Summarize this dialog:\n",
        "\n",
        "#Person1#: This is Person1 part of the conversation.\n",
        "#Person2#: This is Person2 part of the conversation.\n",
        "---\n",
        "Summary:\n",
        "This is ground truth summary of the dialog.\n",
        "```\n",
        "\n",
        "We will create a prompt template and a function to apply the template to all the samples in the original DialogSum dataset. Note that we also append a eos token to the end of the sample. This is so that the fine-tuned model will learn to end the sentence at the appropriate time (e.g. end of the summary) instead of generating tokens indefinitely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15fa67b5-4a13-4c74-9f7f-a4163f90ae95",
      "metadata": {
        "tags": [],
        "id": "15fa67b5-4a13-4c74-9f7f-a4163f90ae95"
      },
      "outputs": [],
      "source": [
        "def apply_prompt_template(sample):\n",
        "    prompt = (\n",
        "        f\"Summarize this dialog:\\n{{dialog}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"text\": prompt.format(\n",
        "            dialog=sample[\"dialogue\"],\n",
        "            summary=sample[\"summary\"],\n",
        "            eos_token=tokenizer.eos_token,\n",
        "        )\n",
        "    }\n",
        "\n",
        "dataset_train = dataset_train.map(apply_prompt_template, remove_columns=list(dataset_train.features))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6b3ad5-e1ca-40ed-af2e-a042b4e14f2f",
      "metadata": {
        "id": "ea6b3ad5-e1ca-40ed-af2e-a042b4e14f2f"
      },
      "source": [
        "Let's look at one of the sample. We can see that the original sample has been converted to sample with a single 'text' field, and the text now confirms to the template we specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df57feef-dccd-4170-83c1-50e2f917a660",
      "metadata": {
        "tags": [],
        "id": "df57feef-dccd-4170-83c1-50e2f917a660"
      },
      "outputs": [],
      "source": [
        "dataset_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66cc3398-53ac-40a0-9c01-5cb5f02e0bd8",
      "metadata": {
        "id": "66cc3398-53ac-40a0-9c01-5cb5f02e0bd8"
      },
      "source": [
        "Similarly we will apply the prompt template to the validation and test splits too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69930bd2-f847-433d-8144-c7a2fa732b15",
      "metadata": {
        "tags": [],
        "id": "69930bd2-f847-433d-8144-c7a2fa732b15"
      },
      "outputs": [],
      "source": [
        "dataset_val = dataset_val.map(apply_prompt_template, remove_columns=list(dataset_val.features))\n",
        "dataset_test = dataset_test.map(apply_prompt_template, remove_columns=list(dataset_test.features))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7e5b558-0dfc-43de-968a-0865f7d35c4b",
      "metadata": {
        "id": "d7e5b558-0dfc-43de-968a-0865f7d35c4b"
      },
      "source": [
        "### Tokenization and Preparing the Input"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a283c3-0209-4a3a-a26c-875c2e0b9bf5",
      "metadata": {
        "id": "40a283c3-0209-4a3a-a26c-875c2e0b9bf5"
      },
      "source": [
        "#### Tokenization\n",
        "\n",
        "Before we can use the dataset for training, we first need to tokenize the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92077d4b-bceb-4b5a-81a4-74022d1ec96e",
      "metadata": {
        "tags": [],
        "id": "92077d4b-bceb-4b5a-81a4-74022d1ec96e"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "dataset_train_tokenized = dataset_train.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=dataset_train.features,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2521c5e7-8976-473a-b7ce-e5bd1b454ab6",
      "metadata": {
        "tags": [],
        "id": "2521c5e7-8976-473a-b7ce-e5bd1b454ab6"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset info: \", dataset_train_tokenized)\n",
        "print(\"Length of input_ids: \", len(dataset_train_tokenized['input_ids'][0]))\n",
        "print(\"Sample input: \\n\", dataset_train_tokenized[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "967dee0e-3fa3-4388-a32e-6a3cf118bccb",
      "metadata": {
        "id": "967dee0e-3fa3-4388-a32e-6a3cf118bccb"
      },
      "source": [
        "We can see that after tokenization, we now have input_ids (which contains the id corresponding to a token (subword), and the attention mask, the attention mask tells the model which token to ignore (e.g. padding). We also shown the input_ids length of the first sample, which in this case is 341 (token ids)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4942aa44-4850-45f1-9a00-932a9ba3d9a1",
      "metadata": {
        "id": "4942aa44-4850-45f1-9a00-932a9ba3d9a1"
      },
      "source": [
        "We will do the same tokenization on our validation dataset and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617a9913-45d2-4bde-917a-e39df9cb291c",
      "metadata": {
        "tags": [],
        "id": "617a9913-45d2-4bde-917a-e39df9cb291c"
      },
      "outputs": [],
      "source": [
        "dataset_val_tokenized = dataset_val.map(\n",
        "    tokenize_function,\n",
        "    batched=True,   # default batch size is 1000\n",
        "    num_proc=4,\n",
        "    remove_columns=dataset_val.features,\n",
        ")\n",
        "\n",
        "dataset_test_tokenized = dataset_test.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=dataset_test.features,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a143a35f-901b-4543-bd1b-2b31e62f3fc4",
      "metadata": {
        "id": "a143a35f-901b-4543-bd1b-2b31e62f3fc4"
      },
      "source": [
        "Now let's prepare the input data to the moodel. As you can see above, typically the length of the token ids (input_ids) are few hundred tokens long. However, Llama model typically have 2048 or 4096 context window. To use the data more efficiently, we use a technique called packing: instead of having one text per sample in the batch and then padding to either the longest text or the maximal context of the model, we concatenate a lot of texts with a EOS token in between and cut chunks of the context size to fill the batch without any padding.\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107/blob/main/session-7/resources/packing.png?raw=1\" width=\"700\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb53e5ba-f34a-46bc-8f6b-0a0414a97bd3",
      "metadata": {
        "id": "cb53e5ba-f34a-46bc-8f6b-0a0414a97bd3"
      },
      "source": [
        "The code below help us find the maximum context window of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "310776bb-8f44-4199-925a-031d271c53bd",
      "metadata": {
        "tags": [],
        "id": "310776bb-8f44-4199-925a-031d271c53bd"
      },
      "outputs": [],
      "source": [
        "def get_max_context_length(model):\n",
        "\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max context lenth: {max_length} in {length_setting}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max context length: {max_length}\")\n",
        "\n",
        "    return max_length\n",
        "\n",
        "max_context_length = get_max_context_length(model)\n",
        "print('Maximum Context length: ', max_context_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f75a8059-64c8-41dd-a083-4171dcb4cd7c",
      "metadata": {
        "id": "f75a8059-64c8-41dd-a083-4171dcb4cd7c"
      },
      "source": [
        "The following functions concatenate a batch of samples, and then divide the concatenated sample into chunks of context size.  Also we also need to create 'labels' in the input dataset, which tells the model what is the token to be predicted.  Shifting the inputs and labels to align them happens inside the model, so our labels are just the exact copy of the input_ids.\n",
        "\n",
        "In the code below, we use a context_length of 1024 instad of the maximum 4096, as we have limited gpu memory and using a larger context length will result in Out of Memory error even with batch size of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd6d2b52-fdec-4c36-9be4-c9922dcb62f3",
      "metadata": {
        "tags": [],
        "id": "bd6d2b52-fdec-4c36-9be4-c9922dcb62f3"
      },
      "outputs": [],
      "source": [
        "context_length = 512\n",
        "# context_length = max_context_length\n",
        "\n",
        "def group_texts(examples):\n",
        "\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    if total_length >= context_length:\n",
        "        total_length = (total_length // context_length) * context_length\n",
        "    # Split by chunks of context length.\n",
        "    result = {\n",
        "        k: [t[i : i + context_length] for i in range(0, total_length, context_length)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de68071b-2cc9-4931-abbc-cb495c83f538",
      "metadata": {
        "tags": [],
        "id": "de68071b-2cc9-4931-abbc-cb495c83f538"
      },
      "outputs": [],
      "source": [
        "dataset_train_final = dataset_train_tokenized.map(group_texts, batched=True, num_proc=4)\n",
        "dataset_val_final = dataset_val_tokenized.map(group_texts, batched=True, num_proc=4)\n",
        "dataset_test_final = dataset_test_tokenized.map(group_texts, batched=True, num_proc=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceafcf63-dbb1-4c60-8a83-ef3417b8ac19",
      "metadata": {
        "id": "ceafcf63-dbb1-4c60-8a83-ef3417b8ac19"
      },
      "source": [
        "Now let's examine the dataset_train_final and we can see that all the samples are of lenghth equal to the specified context window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b28c5b2-4137-4dd6-b071-5894627cc917",
      "metadata": {
        "tags": [],
        "id": "6b28c5b2-4137-4dd6-b071-5894627cc917"
      },
      "outputs": [],
      "source": [
        "dataset_train_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7623184b-18b0-4dfd-84be-2f2750872c77",
      "metadata": {
        "tags": [],
        "id": "7623184b-18b0-4dfd-84be-2f2750872c77"
      },
      "outputs": [],
      "source": [
        "for sample in dataset_train_final['input_ids'][:5]:\n",
        "    print(len(sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8358ebb-45eb-48b9-93b5-7cb4341f62e4",
      "metadata": {
        "id": "b8358ebb-45eb-48b9-93b5-7cb4341f62e4"
      },
      "source": [
        "Since we have done all the heavy lifting of preprocessing the data in our codes, we just use a simple default data collator which basically just pass the dictionary-like input to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "376d35d2-d3e2-4aed-9bb5-08606cddf7ad",
      "metadata": {
        "tags": [],
        "id": "376d35d2-d3e2-4aed-9bb5-08606cddf7ad"
      },
      "outputs": [],
      "source": [
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd17bc21-0252-4436-a462-924e22934286",
      "metadata": {
        "id": "bd17bc21-0252-4436-a462-924e22934286"
      },
      "source": [
        "## Setup the PEFT/LoRA model for Fine-Tuning\n",
        "\n",
        "You need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank $r$ hyper-parameter, which defines the rank/dimension of the adapter to be trained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79bbcc28-be35-4e17-a6ec-d5e5d7695c80",
      "metadata": {
        "tags": [],
        "id": "79bbcc28-be35-4e17-a6ec-d5e5d7695c80"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "\n",
        "def create_peft_config(model):\n",
        "    from peft import (\n",
        "        get_peft_model,\n",
        "        LoraConfig,\n",
        "        TaskType,\n",
        "        prepare_model_for_kbit_training,\n",
        "    )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        inference_mode=False,\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules = [\"q_proj\", \"v_proj\"]\n",
        "    )\n",
        "\n",
        "    # prepare int-8 model for training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    return model, peft_config\n",
        "\n",
        "# create peft config\n",
        "model, lora_config = create_peft_config(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86ed071c-3ab7-4e1c-b032-dc3429907002",
      "metadata": {
        "id": "86ed071c-3ab7-4e1c-b032-dc3429907002"
      },
      "source": [
        "If you look at the trainable prarameters, there are only about 4 million parameters, comparaed to about 6.7 billion parameters of the entire model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc3ead9-0890-4526-90d1-1e98909430dc",
      "metadata": {
        "tags": [],
        "id": "8bc3ead9-0890-4526-90d1-1e98909430dc"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ccc5b40-95a1-4e06-aa3d-67b2b09a6265",
      "metadata": {
        "id": "0ccc5b40-95a1-4e06-aa3d-67b2b09a6265"
      },
      "source": [
        "## Define the Trainer and Training Arguments\n",
        "\n",
        "We can now define training arguments and create Trainer instance. If you are using Ampere GPU (e.g. NVIDIA A10), then you can set bf16 to True to use bfloat16 for mixed precision computation.\n",
        "\n",
        "*Note: Due to long training time (approximately 1 to 2 hours) to fine-tune the model for it to have decent performance, for this lab, we just train for a single step due to time constraint. If you have access to GPUs such a A10G or others, you can train for more steps e.g. 100 steps, and set the logging_steps=10 and save_steps=10 to log and save every 10 steps.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10853496-3414-4176-875c-788e5707b8e4",
      "metadata": {
        "tags": [],
        "id": "10853496-3414-4176-875c-788e5707b8e4"
      },
      "outputs": [],
      "source": [
        "# specify where to write the checkpoint to\n",
        "output_dir = \"train_out_dir\"\n",
        "\n",
        "# Define training args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    auto_find_batch_size=False,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    bf16=False,  # Use BF16 if available (e.g. on Ampere GPU)\n",
        "    # logging strategy\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    # logging_steps=10,\n",
        "    logging_steps=1,\n",
        "    # saving strategy\n",
        "    save_strategy=\"steps\",\n",
        "    # save_steps=10,\n",
        "    save_steps=1,\n",
        "    evaluation_strategy ='steps',\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    load_best_model_at_end=True,\n",
        "    max_steps=1\n",
        ")\n",
        "\n",
        " # Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_train_final,\n",
        "    eval_dataset=dataset_val_final,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3f26e1a-ed52-422c-a3c3-f3495208c87e",
      "metadata": {
        "tags": [],
        "id": "b3f26e1a-ed52-422c-a3c3-f3495208c87e"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93524441-36a1-4f7e-8307-b855e68c5c14",
      "metadata": {
        "tags": [],
        "id": "93524441-36a1-4f7e-8307-b855e68c5c14"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "# trainer.evaluate(eval_dataset=dataset_val_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "825728df-3d1e-4beb-876e-8de547077cfa",
      "metadata": {
        "tags": [],
        "id": "825728df-3d1e-4beb-876e-8de547077cfa"
      },
      "source": [
        "### Save the Trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0595e2-555a-469d-9381-f2fb3c165873",
      "metadata": {
        "tags": [],
        "id": "9d0595e2-555a-469d-9381-f2fb3c165873"
      },
      "outputs": [],
      "source": [
        "save_dir = 'lora_model_output'\n",
        "model.save_pretrained(save_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77c2c46e-964b-4599-8943-6850aabf3af7",
      "metadata": {
        "id": "77c2c46e-964b-4599-8943-6850aabf3af7"
      },
      "source": [
        "### Load the PEFT Model\n",
        "\n",
        "Uncomment the following to download fine-tuned LoRA weights.\n",
        "\n",
        "You should **restart the session to clear the GPU memory** before continuning with the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6bca34f-1e7c-4df6-acf3-d1a534615c6e",
      "metadata": {
        "tags": [],
        "id": "a6bca34f-1e7c-4df6-acf3-d1a534615c6e"
      },
      "outputs": [],
      "source": [
        "!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/pretrained-weights/lora_model_output.zip\n",
        "!unzip  -o lora_model_output.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0cd54961-e4a8-418e-abe3-7797827269b7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import default_data_collator, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import evaluate"
      ],
      "id": "0cd54961-e4a8-418e-abe3-7797827269b7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4856bd-bfb3-48c2-817a-b9ccb271c449",
      "metadata": {
        "tags": [],
        "id": "bb4856bd-bfb3-48c2-817a-b9ccb271c449"
      },
      "outputs": [],
      "source": [
        "model_id = 'meta-llama/Llama-2-7b-hf'\n",
        "save_dir = 'lora_model_output'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "peft_model = AutoModelForCausalLM.from_pretrained(save_dir, device_map='cuda:0', load_in_8bit=True, torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b56996a5-c0ab-4bfb-8c84-b267944702ab",
      "metadata": {
        "tags": [],
        "id": "b56996a5-c0ab-4bfb-8c84-b267944702ab"
      },
      "outputs": [],
      "source": [
        "peft_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6645dc1-c797-4044-84c5-be6955d66252",
      "metadata": {
        "id": "d6645dc1-c797-4044-84c5-be6955d66252"
      },
      "source": [
        "### Test the Model\n",
        "\n",
        "Now let's test our fine-tuned model on the same prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c96ae5df-3f35-459e-be1b-0c6bb9ed487a",
      "metadata": {
        "tags": [],
        "id": "c96ae5df-3f35-459e-be1b-0c6bb9ed487a"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"\n",
        "Summarize this dialog:\n",
        "#Person1#: Hello, how are you doing today?\n",
        "#Person2#: I ' Ve been having trouble breathing lately.\n",
        "#Person1#: Have you had any type of cold lately?\n",
        "#Person2#: No, I haven ' t had a cold. I just have a heavy feeling in my chest when I try to breathe.\n",
        "#Person1#: Do you have any allergies that you know of?\n",
        "#Person2#: No, I don ' t have any allergies that I know of.\n",
        "#Person1#: Does this happen all the time or mostly when you are active?\n",
        "#Person2#: It happens a lot when I work out.\n",
        "#Person1#: I am going to send you to a pulmonary specialist who can run tests on you for asthma.\n",
        "#Person2#: Thank you for your help, doctor.\n",
        "---\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "# eval_prompt = \"\"\"\n",
        "# Summarize this dialog:\n",
        "# A: Hi Tom, are you busy tomorrow’s afternoon?\n",
        "# B: I’m pretty sure I am. What’s up?\n",
        "# A: Can you go with me to the animal shelter?.\n",
        "# B: What do you want to do?\n",
        "# A: I want to get a puppy for my son.\n",
        "# B: That will make him so happy.\n",
        "# A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
        "# B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
        "# A: I'll get him one of those little dogs.\n",
        "# B: One that won't grow up too big;-)\n",
        "# A: And eat too much;-))\n",
        "# B: Do you know which one he would like?\n",
        "# A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
        "# B: I bet you had to drag him away.\n",
        "# A: He wanted to take it home right away ;-).\n",
        "# B: I wonder what he'll name it.\n",
        "# A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
        "# ---\n",
        "# Summary:\n",
        "# \"\"\"\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# #Streaming support\n",
        "# streamer = TextStreamer(tokenizer)\n",
        "# peft_model.generate(**model_input, streamer=streamer)\n",
        "peft_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(peft_model.generate(**model_input)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c28b4579-06c0-432b-bbad-e34827175d3e",
      "metadata": {
        "id": "c28b4579-06c0-432b-bbad-e34827175d3e"
      },
      "source": [
        "## Evaluate the model using ROUGE metric"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43e83fe6-fc35-486b-9d48-cccb541b3afe",
      "metadata": {
        "id": "43e83fe6-fc35-486b-9d48-cccb541b3afe"
      },
      "source": [
        "We first define some utility function to extract the summary part from the dialog summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1498e82d-ecb6-47f7-ad98-2381bed86417",
      "metadata": {
        "tags": [],
        "id": "1498e82d-ecb6-47f7-ad98-2381bed86417"
      },
      "outputs": [],
      "source": [
        "# remove the dialog and retain only text in the summary\n",
        "def get_summary(text):\n",
        "    parts = re.split(r'Summary:', text)\n",
        "    summary = parts[1].strip()\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d4924b2-4a95-4c9f-afaf-c68405229d65",
      "metadata": {
        "id": "4d4924b2-4a95-4c9f-afaf-c68405229d65"
      },
      "source": [
        "The original test set has 1500 entries, and it will take a long time to compute the rouge.  To speed up things, we just compute ROUGE for the first 15 test samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a0602d-47a3-4e1f-aa3b-80631d9398b9",
      "metadata": {
        "tags": [],
        "id": "32a0602d-47a3-4e1f-aa3b-80631d9398b9"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(dataset_name)\n",
        "dialogues = dataset['test']['dialogue'][:15]\n",
        "human_baseline_summaries = dataset['test']['summary'][:15]\n",
        "peft_model_summaries = []\n",
        "\n",
        "for _, dialogue in enumerate(dialogues):\n",
        "    eval_prompt = f\"\"\"\n",
        "Summarize this dialog:\n",
        "{dialogue}\n",
        "---\n",
        "Summary:\n",
        "\"\"\"\n",
        "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        peft_model_output = tokenizer.decode(peft_model.generate(**model_input)[0], skip_special_tokens=True)\n",
        "    summary = get_summary(peft_model_output)\n",
        "    peft_model_summaries.append(summary)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd56caaf-47bb-454d-8046-a20a54a93eca",
      "metadata": {
        "tags": [],
        "id": "dd56caaf-47bb-454d-8046-a20a54a93eca"
      },
      "outputs": [],
      "source": [
        "print('Human Baseline')\n",
        "print('*'*10)\n",
        "for summary in human_baseline_summaries[:5]:\n",
        "    print(summary)\n",
        "print('PEFT summaries')\n",
        "print('*'*10)\n",
        "for summary in peft_model_summaries[:5]:\n",
        "    print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a42d98a-ae8b-4453-88b2-94ec290e9529",
      "metadata": {
        "tags": [],
        "id": "2a42d98a-ae8b-4453-88b2-94ec290e9529"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "print('PEFT model ROUGE scores:')\n",
        "print(peft_model_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9d3849b-0d22-49a2-86cb-9f8330faed90",
      "metadata": {
        "id": "b9d3849b-0d22-49a2-86cb-9f8330faed90"
      },
      "source": [
        "### Testing the token generation speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41045d0e-e5ec-45b0-8cc1-2c9294a93a18",
      "metadata": {
        "tags": [],
        "id": "41045d0e-e5ec-45b0-8cc1-2c9294a93a18"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import time\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=peft_model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b830580a-ac0d-439e-a06b-bdc38f980f41",
      "metadata": {
        "tags": [],
        "id": "b830580a-ac0d-439e-a06b-bdc38f980f41"
      },
      "outputs": [],
      "source": [
        "tokens_per_second_list = []\n",
        "\n",
        "for i in range(20):\n",
        "    start = time.time()\n",
        "    output = pipeline(eval_prompt, max_new_tokens=30, temperature=1, top_k=1, top_p=0.90)\n",
        "\n",
        "    delay = time.time()\n",
        "    total_time = (delay - start)\n",
        "    time_per_token = total_time / 30\n",
        "\n",
        "    # Calculate tokens per second\n",
        "    tokens_per_second = 30 / total_time\n",
        "    tokens_per_second_list.append(tokens_per_second)\n",
        "\n",
        "\n",
        "average = sum(tokens_per_second_list) / len(tokens_per_second_list)\n",
        "# Print the results\n",
        "print(\"Total inference time: {:.2f} ms\".format(total_time))\n",
        "print(\"Time per token: {:.2f} ms/token\".format(time_per_token))\n",
        "print(\"Tokens per second: {:.2f} token/s\".format(average))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aabba43-ed35-446e-9081-579bd117ac86",
      "metadata": {
        "id": "5aabba43-ed35-446e-9081-579bd117ac86"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}